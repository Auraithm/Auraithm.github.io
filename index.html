<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Aha - A High-Performance and Efficient Diffusion Language Model - Ying Zhu, Jiaxin Wan, Tianyi Liang, Xu Guo, Xiaoran Liu, Zengfeng Huang, Ziwei He, Xipeng Qiu">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Ying Zhu, Jiaxin Wan, Tianyi Liang, Xu Guo, Xiaoran Liu, Zengfeng Huang, Ziwei He, Xipeng Qiu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Aha - A High-Performance and Efficient Diffusion Language Model">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Ying Zhu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Aha - A High-Performance and Efficient Diffusion Language Model">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Aha - A High-Performance and Efficient Diffusion Language Model">
  <meta name="citation_author" content="Zhu, Ying">
  <meta name="citation_author" content="Wan, Jiaxin">
  <meta name="citation_author" content="Liang, Tianyi">
  <meta name="citation_author" content="Guo, Xu">
  <meta name="citation_author" content="Liu, Xiaoran">
  <meta name="citation_author" content="Huang, Zengfeng">
  <meta name="citation_author" content="He, Ziwei">
  <meta name="citation_author" content="Qiu, Xipeng">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Aha - A High-Performance and Efficient Diffusion Language Model - Ying Zhu et al. | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Aha - A High-Performance and Efficient Diffusion Language Model",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "Ying Zhu",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Jiaxin Wan",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Electronic Science and Technology of China, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Tianyi Liang",
        "affiliation": {
          "@type": "Organization",
          "name": "East China Normal University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Xu Guo",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Xiaoran Liu",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Zengfeng Huang",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Ziwei He",
        "affiliation": {
          "@type": "Organization",
          "name": "Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Xipeng Qiu",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Aha: A High-Performance and Efficient Diffusion Language Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="#" target="_blank">Ying Zhu</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Jiaxin Wan</a><sup>3,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Tianyi Liang</a><sup>4,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Xu Guo</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Xiaoran Liu</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Zengfeng Huang</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Ziwei He</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Xipeng Qiu</a><sup>1,2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Fudan University, <sup>2</sup>Shanghai Innovation Institute,<br><sup>3</sup>University of Electronic Science and Technology of China, <br><sup>4</sup>East China Normal University<br><br>Conference name and year</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- GitHub Link -->
                      <span class="link-block">
                        <a href="https://github.com/YOUR_REPO_HERE" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <img src="https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png" alt="GitHub" style="width: 1.2em; height: 1.2em; vertical-align: middle; filter: invert(1);">
                        </span>
                        <span>GitHub</span>
                      </a>
                    </span>

                    <!-- Hugging Face Link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/YOUR_MODEL_HERE" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo.svg" alt="Hugging Face" style="width: 1.2em; height: 1.2em; vertical-align: middle;">
                      </span>
                      <span>Hugging Face</span>
                    </a>
                  </span>

                  <!-- Paper Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/YOUR_PAPER_ID.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Main Teaser Image -->
      <figure class="image">
        <img src="static/images/accuracy.png" alt="Aha Framework Overview" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 16px rgba(0,0,0,0.15);"/>
      </figure>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- TL;DR -->
<section class="hero is-small"  style="padding: 8rem 5rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">TL;DR</h2>
        <div class="content has-text-left" style="font-size: 1.2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.7;">
          <p style="margin-bottom: 1rem;">We introduce <strong style="font-weight: 600;">Aha</strong>, a high-performance open-source diffusion large language model (DLLM), alongside an open-source post-training framework specifically designed for efficient and scalable DLLM adaptation. Our framework supports long-context post-training up to 8K tokens, dramatically reducing computational cost while preserving model stability and convergence.</p>
          <p>Built upon this framework, Aha achieves state-of-the-art (SOTA) results among models of similar parameter scale, surpassing existing post-training approaches in both efficiency and generalization. This release establishes a practical, fully open foundation for advancing long-context DLLM post-training and adaptation research.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End TL;DR -->


<!-- HighLights Section -->
<section class="hero is-small is-light" style="padding: 5rem 1.5rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 0.5rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">HighLights</h2>
    <div class="content" style="font-size: 1.2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.7;">
      <ul style="line-height: 1.8; margin-bottom: 0; list-style-position: outside; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.75rem;"><strong style="font-weight: 600;">üöÄ Training ‚Äî Efficient Post-Training Framework:</strong> An open-source post-training framework specifically designed for DLLMs.</li>
        <li style="margin-bottom: 0.75rem;"><strong style="font-weight: 600;">‚ö° Speed ‚Äî Parallel Decoding:</strong> Up to 32√ó speedup in inference time.</li>
        <li><strong style="font-weight: 600;">üß† Performance ‚Äî Advanced Science Reasoning Benchmarks:</strong> High scores on MATH and AIME.</li>
      </ul>
    </div>
  </div>
</section>
<!-- End HighLights Section -->













<!-- Teaser trace-->
<section class="hero teaser" style="padding: 3rem 5rem;">
  <div class="container" style="max-width: 1000px !important; width: 1000px !important;">
    <div class="hero-body">
        <style>
        :root {
            --fg: #111;
            --bg: #fafafa;
            --muted: #888;
            --new: #0a7f2e;
            --mask: #aaa;
            --border: #ddd;
            --prompt-bg: transparent;
            --output-bg: #fdfdfd;
            --prompt-border: #e0e0e0;
            --output-border: #c4e3c4;
        }
        .teaser .hero-body {
            font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
            color: var(--fg);
        }
        .teaser h1 {
            font-size: 20px;
            margin: 0 0 12px 0;
            font-weight: 600;
            text-align: center;
            color: #333;
        }
        .teaser #meta {
            color: var(--muted);
            font-size: 13px;
            margin-bottom: 10px;
            text-align: center;
        }
        .teaser .section {
            border-radius: 12px;
            padding: 16px;
            margin-bottom: 20px;
            white-space: pre-wrap;
            overflow-wrap: anywhere;
            line-height: 1.55;
            box-shadow: 0 1px 2px rgba(0,0,0,0.05);
            width: 1000px !important;
            max-width: 1000px !important;
            box-sizing: border-box;
        }
        .teaser #prompt {
            background: var(--prompt-bg);
            border: 1px solid var(--prompt-border);
            min-height: 80px;
            padding: 16px 16px !important;
            width: 1000px !important;
        }
        .teaser #output {
            background: var(--output-bg);
            border: 1px solid var(--output-border);
            min-height: 800px;
            height: 800px;
            overflow: hidden;
            padding: 16px 16px !important;
            width: 1000px !important;
        }
        .teaser .masked {
            color: transparent;
            text-shadow: 0 0 0 var(--mask);
        }
        .teaser .unmasked {
            color: inherit;
        }
        .teaser .new {
            background: #e9f7ee;
            outline: 1px dashed #b7e1c3;
            animation: fadeIn 0.3s ease-out;
        }
        .teaser .prompt {
            color: #6b7280;
            font-weight: 500;
        }
        .teaser .special {
            color: var(--muted);
        }
        .teaser .maskToken {
            display:inline-block;
            width:9ch;
            text-align:center;
            opacity:0.6;
            white-space:nowrap;
        }
        @keyframes fadeIn {
            from { background:#cdecd8; }
            to { background:#e9f7ee; }
        }
        </style>

        <h2 class="title is-3 has-text-centered" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin-bottom: 2rem;">Decoding Visualization</h2>
        <div id="meta"></div>
        
        <div id="prompt" class="section"></div>
        <div id="output" class="section"></div>

        <script id="data" type="application/json">{"pieces": ["Prompt", ":", " ", "<|im_start|>", "user", "\n", "Find", " the", " sum", " of", " all", " integer", " bases", " $", "b", ">", "9", "$", " for", " which", " $", "1", "7", "_{", "b", "}$", " is", " a", " divisor", " of", " $", "9", "7", "_{", "b", "}$", ".\n", "Please", " reason", " step", " by", " step", ",", " and", " put", " your", " final", " answer", " within", " $\\", "boxed", "{}", "$.", "<|im_end|>", "\n", "<|im_start|>", "assistant", "\n", "Alright", ",", " I", " have", " this", " problem", ":\n\n", "**", "Find", " the", " sum", " of", " all", " integer", " bases", " \\(", " b", " >", " ", "9", " \\", ")", " for", " which", " \\(", " ", "1", "7", "_b", " \\", ")", " is", " a", " divisor", " of", " \\(", " ", "9", "7", "_b", " \\", ").", "**\n\n", "First", ",", " I", " need", " to", " understand", " what", " \\(", " ", "1", "7", "_b", " \\", ")", " and", " \\(", " ", "9", "7", "_b", " \\", ")", " mean", ".", " The", " subscript", " \\(", " b", " \\", ")", " indicates", " that", " these", " numbers", " are", " in", " base", " \\(", " b", " \\", ").", " \n\n", "###", " Step", " ", "1", ":", " Convert", " Base", " \\(", " b", " \\", ")", " Numbers", " to", " Decimal", "\n", "I", " need", " to", " express", " both", " numbers", " in", " terms", " of", " base", " ", "1", "0", " (", "decimal", ")", " to", " work", " with", " them", " easily", ".\n\n", "1", ".", " **", "Convert", " \\(", " ", "1", "7", "_b", " \\", ")", " to", " decimal", ":", "**\n", "  ", " \\", "[", " ", "1", "7", "_b", " =", " ", "1", " \\", "times", " b", "^", "1", " +", " ", "7", " \\", "times", " b", "^", "0", " =", " b", " +", " ", "7", " \\", "]\n\n", "2", ".", " **", "Convert", " \\(", " ", "9", "7", "_b", " \\", ")", " to", " decimal", ":", "**\n", "  ", " \\", "[", " ", "9", "7", "_b", " =", " ", "9", " \\", "times", " b", "^", "1", " +", " ", "7", " \\", "times", " b", "^", "0", " =", " ", "9", "b", " +", " ", "7", " \\", "]\n\n", "###", " Step", " ", "2", ":", " Understand", " the", " Div", "is", " Condition", "\n", "The", " problem", " states", " that", " \\(", " ", "1", "7", "_b", " \\", ")", " is", " a", " divisor", " of", " \\(", " ", "9", "7", "_b", " \\", ").", " In", " decimal", " terms", ",", " means", ":\n", "\\[", " b", " +", " ", "7", " \\", "text", "{", " divides", " }", " ", "9", "b", " +", " ", "7", " \\", "]\n\n", "In", " other", " words", ",", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " is", " divisible", " exactly", " by", " \\(", " b", " +", " ", "7", " \\", ").", " \n\n", "###", " Step", " ", "3", ":", " Use", " Polynomial", " Division", " or", " Remain", "der", " The", "orem", "\n", "I", " recall", " that", " for", " divis", "ibility", " conditions", " like", " this", ",", " it", "'s", " helpful", " to", " perform", " polynomial", " division", " or", " use", " the", " Remain", "der", " The", "orem", ".\n\n", "**", "Let", "'s", " perform", " polynomial", " division", ":", "**\n\n", "Div", "ide", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " by", " \\(", " b", " +", " ", "7", " \\", ").\n\n", "1", ".", " **", "First", " division", " step", ":", "**\n", "  ", " \\", "[", " \\", "frac", "{", "9", "b", "}{", "b", "}", " =", " ", "9", " \\", "]\n", "  ", " Multiply", " \\(", " ", "9", " \\", ")", " by", " \\(", " b", " +", " ", "7", " \\", "):", " \\(", " ", "9", "b", " +", " ", "6", "3", " \\", ")\n", "  ", " Subtract", " from", " \\(", " ", "9", "b", " +", " ", "7", " \\", "):\n", "  ", " \\", "[", " (", "9", "b", " +", " ", "7", ")", " -", " (", "9", "b", " +", " ", "6", "3", ")", " =", " -", "5", "6", " \\", "]\n\n", "So", ",", " the", " remainder", " is", " \\", "(-", "5", "6", "\\", ").\n\n", "But", " since", " \\(", " b", " +", " ", "7", " \\", ")", " must", " divide", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " exactly", ",", " the", " remainder", " must", " be", " zero", ".", " However", ",", " here", " the", " remainder", " is", " \\", "(-", "5", "6", "\\", "),", " which", " is", " not", " zero", ".", " \n\n", "**", "But", " this", " seems", " contradictory", "!", "**", " The", " condition", " says", " that", " \\(", " b", " +", " ", "7", " \\", ")", " divides", " \\(", " ", "9", "b", " +", " ", "7", " \\", "),", " but", " according", " to", " the", " division", ",", " the", " remainder", " is", " non", "-zero", ".", " \n\n", "###", " Step", " ", "4", ":", " Re", "-e", "valuate", " the", " Approach", "\n", "Maybe", " I", " made", " a", " mistake", ".", " Instead", " of", " division", ",", " perhaps", " I", " should", " think", " about", " the", " condition", " in", " terms", " of", " divis", "ibility", ".\n\n", "The", " condition", " \\(", " b", " +", " ", "7", " \\", ")", " divides", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " can", " be", " written", " as", ":\n", "\\[", " ", "9", "b", " +", " ", "7", " \\", "equiv", " ", "0", " \\", "mod", " (", " (", "b", " +", " ", "7", "))", " \\", "]\n\n", "But", " from", " the", " division", ",", " we", " saw", " that", ":\n", "\\[", " ", "9", "b", " +", " ", "7", " =", " (", "b", " +", " ", "7", ")", " \\", "times", " ", "9", " +", " (-", "5", "6", ")", " \\", "]\n\n", "Thus", ":\n", "\\[", " ", "9", "b", " +", " ", "7", " \\", "equiv", " -", "5", "6", " \\", "mod", " (", "b", " +", " ", "7", ")", " \\", "]\n\n", "But", " \\(", " b", " +", " ", "7", " \\", "equiv", " ", "0", " \\", "mod", " (", "b", " +", " ", "7", ")", " \\", "),", " so", ":\n", "\\[", " -", "5", "6", " \\", "equiv", " ", "0", " \\", "mod", " (", "b", " +", " ", "7", ")", " \\", "]\n\n", "Which", " implies", ":\n", "\\[", " b", " +", " ", "7", " \\", "text", "{", " divides", " }", " ", "5", "6", " \\", "]\n\n", "###", " Step", " ", "5", ":", " Find", " All", " Div", "isors", " of", " ", "5", "6", "\n", "Now", ",", " I", " need", " to", " find", " all", " positive", " div", "isors", " of", " ", "5", "6", " because", " \\(", " b", " +", " ", "7", " \\", ")", " must", " be", " positive", " (", "since", " \\(", " b", " >", " ", "9", " \\", "))", ".\n\n", "First", ",", " factor", "ize", " ", "5", "6", ":\n", "\\[", " ", "5", "6", " =", " ", "2", "^", "3", " \\", "times", " ", "7", " \\", "]\n\n", "The", " positive", " div", "isors", " are", ":\n", "\\[", " ", "1", ",", " ", "2", ",", " ", "4", ",", " ", "7", ",", " ", "8", ",", " ", "1", "4", ",", " ", "2", "8", ",", " ", "5", "6", " \\", "]\n\n", "###", " Step", " ", "6", ":", " Determine", " Correspond", "ing", " Values", " of", " \\(", " b", " \\", ")\n", "From", " \\(", " b", " +", " ", "7", " \\", ")", " must", " be", " one", " of", " these", " div", "isors", ",", " so", ":\n", "\\[", " b", " +", " ", "7", " =", " d", " \\", "quad", " \\", "text", "{", "where", " }", " d", " \\", "text", "{", " is", " a", " divisor", " of", " ", "5", "6", "}", " \\", "]\n", "\\[", " b", " =", " d", " -", " ", "7", " \\", "]\n\n", "But", " the", " problem", " specifies", " \\(", " b", " >", " ", "9", " \\", "),", " so", ":\n", "\\[", " d", " -", " ", "7", " >", " ", "9", " \\", "]\n", "\\[", " d", " >", " ", "1", "6", " \\", "]\n\n", "Looking", " at", " the", " div", "isors", ":\n", "-", " \\(", " d", " =", " ", "2", "8", " \\", "):", " \\(", " b", " =", " ", "2", "1", " \\", ")\n", "-", " \\(", " d", " =", " ", "5", "6", " \\", "):", " \\(", " b", " =", " ", "4", "9", " \\", ")\n\n", "Other", " div", "isors", " \\(", " d", " \\", "le", "q", " ", "1", "6", " \\", ")", " would", " give", " \\(", " b", " \\", "le", "q", " ", "9", " \\", "),", " which", " is", " invalid", ".\n\n", "###", " Verification", "\n", "I", " need", " to", " ensure", " that", " for", " these", " \\(", " b", " \\", ")", " values", ",", " the", " original", " condition", " holds", ".\n\n", "**", "For", " \\(", " b", " =", " ", "2", "1", " \\", "):", "**\n", "-", " \\(", " ", "1", "7", "_{", "2", "1", "}", " =", " ", "2", "1", " +", " ", "7", " =", " ", "2", "8", " \\", ")\n", "-", " \\(", " ", "9", "7", "_{", "2", "1", "}", " =", " ", "9", " \\", "times", " ", "2", "1", " +", " ", "7", " =", " ", "1", "8", "9", " +", " ", "7", " =", " ", "1", "9", "6", " \\", ")\n", "-", " Check", " if", " \\(", " ", "2", "8", " \\", ")", " divides", " \\(", " ", "1", "9", "6", " \\", "):", " \\(", " ", "1", "9", "6", " \\", "div", " ", "2", "8", " =", " ", "7", " \\", ").", " Exactly", ".\n\n", "**", "For", " \\(", " b", " =", " ", "4", "9", " \\", "):", "**\n", "-", " \\(", " ", "1", "7", "_{", "4", "9", "}", " =", " ", "4", "9", " +", " ", "7", " =", " ", "5", "6", " \\", ")\n", "-", " \\(", " ", "9", "7", "_{", "4", "9", "}", " =", " ", "9", " \\", "times", " ", "4", "9", " +", " ", "7", " =", " ", "4", "4", "1", " +", " ", "7", " =", " ", "4", "4", "8", " \\", ")\n", "-", " Check", " if", " \\(", " ", "5", "6", " \\", ")", " divides", " \\(", " ", "4", "4", "8", " \\", "):", " \\(", " ", "4", "4", "8", " \\", "div", " ", "5", "6", " =", " ", "8", " \\", ").", " Exactly", ".\n\n", "All", " other", " div", "isors", " \\(", " d", " \\", "le", "q", " ", "1", "6", " \\", ")", " would", " give", " \\(", " b", " \\", "le", "q", " ", "9", " \\", "),", " which", " is", " not", " allowed", ".\n\n", "###", " Sum", " of", " Valid", " B", "ases", "\n", "Only", " \\(", " b", " =", " ", "2", "1", " \\", ")", " and", " \\(", " b", " =", " ", "4", "9", " \\", ")", " satisfy", " the", " condition", " \\(", " b", " >", " ", "9", " \\", ").\n\n", "Now", ",", " sum", " them", ":\n", "\\[", " ", "2", "1", " +", " ", "4", "9", " =", " ", "7", "0", " \\", "]\n\n", "###", " Conclusion", "\n", "The", " sum", " of", " all", " integer", " bases", " \\(", " b", " >", " ", "9", " \\", ")", " for", " which", " \\(", " ", "1", "7", "_b", " \\", ")", " divides", " \\(", " ", "9", "7", "_b", " \\", ")", " is", " \\(", " \\", "boxed", "{", "7", "0", "}", " \\", ").", "<|im_end|>", "<|endoftext|>", "\n"], "step_map": [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, 2, 1, 3, 5, 3, 4, 6, 7, 6, 8, 9, 10, 9, 9, 11, 11, 11, 11, 12, 12, 12, 13, 14, 14, 14, 14, 16, 15, 15, 15, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 36, 37, 37, 37, 39, 38, 39, 39, 40, 40, 40, 40, 41, 41, 41, 41, 42, 43, 44, 44, 45, 45, 45, 46, 47, 48, 50, 49, 51, 53, 54, 52, 55, 58, 56, 57, 59, 60, 61, 62, 63, 63, 63, 63, 65, 64, 65, 65, 66, 66, 67, 68, 69, 70, 70, 70, 71, 72, 73, 71, 74, 74, 74, 74, 75, 75, 76, 77, 78, 79, 80, 81, 82, 82, 82, 83, 84, 85, 87, 86, 88, 88, 88, 89, 90, 90, 91, 92, 94, 93, 96, 95, 99, 97, 98, 100, 101, 104, 102, 103, 105, 105, 105, 105, 106, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 108, 109, 109, 109, 109, 110, 110, 110, 110, 111, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 113, 114, 115, 117, 116, 120, 118, 119, 120, 121, 121, 121, 121, 124, 122, 123, 123, 125, 125, 125, 126, 128, 127, 128, 129, 131, 131, 130, 130, 132, 132, 132, 132, 133, 133, 133, 133, 134, 134, 134, 135, 136, 136, 136, 137, 138, 138, 138, 138, 139, 139, 139, 139, 140, 142, 143, 141, 144, 145, 146, 145, 149, 148, 147, 150, 151, 151, 153, 152, 155, 155, 156, 154, 157, 157, 157, 158, 159, 161, 162, 160, 164, 163, 165, 166, 167, 169, 169, 168, 170, 170, 170, 170, 171, 171, 171, 171, 172, 172, 172, 172, 173, 173, 173, 174, 175, 175, 175, 175, 176, 176, 177, 178, 179, 180, 181, 181, 182, 182, 182, 183, 184, 186, 185, 186, 187, 187, 187, 187, 188, 188, 188, 188, 190, 190, 189, 191, 192, 192, 192, 192, 193, 193, 193, 193, 194, 194, 194, 194, 195, 197, 198, 196, 199, 199, 199, 199, 200, 200, 200, 200, 201, 201, 201, 201, 202, 202, 202, 202, 203, 203, 203, 204, 205, 207, 208, 206, 209, 209, 209, 209, 210, 210, 211, 210, 212, 212, 212, 212, 213, 213, 213, 213, 214, 214, 214, 214, 215, 215, 215, 215, 216, 216, 216, 217, 218, 218, 218, 218, 220, 219, 219, 219, 221, 221, 221, 221, 222, 222, 222, 222, 223, 223, 223, 223, 224, 224, 224, 224, 225, 225, 225, 225, 226, 226, 226, 226, 227, 227, 227, 227, 228, 228, 228, 228, 229, 230, 231, 231, 232, 233, 233, 233, 234, 234, 234, 234, 235, 235, 235, 235, 236, 236, 236, 236, 237, 237, 237, 237, 238, 238, 238, 238, 239, 239, 239, 239, 240, 240, 240, 240, 241, 241, 241, 241, 242, 242, 242, 242, 245, 246, 244, 243, 250, 249, 248, 247, 251, 251, 251, 252, 256, 253, 254, 255, 260, 259, 258, 257, 261, 261, 261, 262, 263, 263, 264, 265, 266, 268, 267, 268, 269, 269, 269, 269, 270, 270, 270, 270, 271, 271, 271, 271, 272, 272, 272, 272, 273, 273, 274, 275, 276, 276, 278, 277, 279, 279, 279, 279, 280, 280, 280, 280, 282, 282, 281, 283, 284, 285, 285, 284, 286, 286, 287, 288, 289, 291, 290, 292, 293, 295, 294, 295, 296, 296, 296, 296, 297, 299, 298, 300, 301, 302, 301, 302, 303, 304, 305, 305, 306, 307, 308, 309, 311, 310, 311, 312, 313, 315, 314, 316, 317, 317, 317, 317, 318, 319, 320, 320, 321, 321, 321, 321, 322, 325, 323, 324, 326, 326, 327, 328, 329, 330, 329, 331, 332, 334, 333, 334, 335, 335, 335, 336, 337, 338, 340, 339, 341, 341, 341, 341, 342, 343, 345, 344, 346, 346, 346, 347, 350, 348, 349, 351, 353, 352, 354, 355, 356, 356, 357, 358, 360, 359, 359, 360, 361, 361, 361, 361, 362, 364, 363, 365, 366, 366, 366, 367, 368, 368, 368, 368, 369, 369, 369, 369, 370, 370, 370, 370, 371, 371, 371, 371, 372, 373, 372, 372, 374, 374, 374, 374, 375, 375, 375, 376, 378, 377, 378, 378, 380, 381, 379, 380, 382, 382, 382, 382, 383, 383, 383, 383, 384, 384, 384, 384, 385, 385, 385, 385, 386, 386, 386, 386, 387, 387, 387, 387, 388, 389, 390, 391, 395, 392, 393, 394, 396, 396, 396, 397, 398, 398, 398, 399, 401, 401, 400, 401, 402, 402, 402, 403, 404, 404, 404, 404, 405, 406, 408, 407, 409, 410, 412, 411, 413, 413, 413, 413, 414, 414, 416, 415, 417, 418, 420, 419, 421, 423, 423, 422, 424, 424, 424, 424, 425, 425, 425, 426, 427, 429, 428, 429, 430, 433, 431, 432, 434, 437, 436, 435, 438, 440, 439, 441, 442, 442, 442, 442, 444, 445, 443, 444, 447, 447, 446, 446, 448, 448, 448, 449, 451, 452, 450, 453, 454, 455, 455, 454, 456, 457, 459, 458, 460, 460, 460, 461, 463, 462, 463, 463, 464, 464, 464, 464, 465, 465, 465, 465, 466, 466, 466, 466, 467, 467, 467, 467, 468, 468, 468, 468, 469, 469, 469, 469, 470, 472, 471, 472, 473, 473, 473, 473, 474, 474, 475, 474, 476, 476, 476, 476, 477, 480, 479, 478, 481, 484, 483, 482, 486, 488, 487, 485, 489, 492, 491, 490, 495, 494, 494, 493, 497, 497, 497, 496, 498, 498, 500, 499, 501, 502, 503, 504, 505, 505, 505, 505, 506, 506, 506, 506, 507, 509, 509, 508, 510, 510, 510, 510, 511, 511, 512, 513, 514, 515, 516, 517, 518, 519, 519, 520, 521, 521, 521, 521, 522, 522, 523, 524, 527, 525, 525, 526, 528, 528, 528, 529, 530, 530, 530, 530, 531, 532, 533, 533, 534, 534, 534, 534, 535, 535, 535, 535, 536, 536, 536, 536, 537, 537, 537, 537, 538, 538, 538, 538, 539, 539, 539, 540, 541, 542, 541, 543, 544, 544, 544, 544, 545, 545, 545, 545, 546, 546, 547, 546, 548, 548, 548, 548, 549, 549, 549, 549, 551, 551, 550, 550, 552, 553, 552, 552, 554, 554, 555, 556, 557, 559, 558, 558, 561, 561, 560, 561, 562, 563, 562, 562, 565, 564, 565, 566, 567, 567, 567, 567, 568, 568, 568, 569, 570, 570, 570, 570, 571, 571, 571, 571, 572, 574, 573, 574, 575, 575, 575, 575, 576, 576, 576, 576, 578, 577, 577, 577, 579, 579, 579, 579, 580, 581, 580, 582, 583, 585, 584, 583, 586, 587, 586, 586, 589, 588, 588, 588, 590, 591, 591, 590, 592, 592, 593, 593, 594, 594, 594, 594, 595, 595, 595, 595, 596, 596, 596, 596, 599, 598, 597, 598, 600, 600, 601, 602, 603, 604, 606, 605, 607, 608, 609, 610, 611, 612, 614, 613, 615, 615, 615, 615, 616, 616, 616, 616, 617, 617, 617, 617, 618, 619, 620, 620, 621, 622, 621, 623, 624, 625, 626, 627, 628, 629, 631, 630, 633, 632, 634, 635, 638, 636, 637, 639, 640, 642, 642, 641, 643, 643, 643, 643, 644, 645, 647, 646, 648, 649, 651, 650, 653, 653, 653, 652, 654, 654, 656, 655, 657, 658, 659, 660, 661, 664, 663, 662, 665, 665, 665, 666, 667, 667, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 679, 678, 680, 681, 682, 681, 683, 685, 685, 684, 685, 686, 686, 686, 687, 688, 689, 689, 689, 690, 690, 691, 692, 696, 694, 695, 693, 697, 697, 697, 698, 699, 700, 700, 700, 701, 701, 701, 701, 702, 704, 703, 705, 708, 707, 706, 706, 709, 711, 710, 712, 713, 714, 715, 716, 717, 717, 717, 717, 718, 718, 718, 718, 719, 719, 719, 719, 720, 720, 720, 720, 721, 721, 722, 723, 725, 724, 726, 726, 727, 730, 728, 729, 731, 732, 732, 732, 733, 733, 733, 733, 735, 734, 735, 736, 737, 739, 739, 738, 740, 740, 741, 742, 743], "is_special": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "prompt_length": 57}</script>
        <script>
            const DATA = JSON.parse(document.getElementById('data').textContent);
            const pieces = DATA.pieces.map(piece => 
            piece.replace(/\f/g, '\\f').replace(/\t/g, '\\t')
                .replace(/\r/g, '\\r').replace(/\v/g, '\\v')
            );
            const steps  = DATA.step_map;
            const isSpec = DATA.is_special;
            const promptLength = DATA.prompt_length;

            const maxStep = Math.max(...steps);
            const promptEl = document.getElementById('prompt');
            const outputEl = document.getElementById('output');
            const meta = document.getElementById('meta');
            let currentStep = 0;

            function render(t) {
            const fragPrompt = document.createDocumentFragment();
            const fragOutput = document.createDocumentFragment();

            let revealed = 0;

            // È¶ñÂÖàÁªüËÆ°Â∑≤ÁªèÁîüÊàêÁöÑ output token Êï∞Èáè
            for (let i = 0; i < pieces.length; i++) {
                if (steps[i] !== -1 && steps[i] <= t) {
                    revealed++;
                }
            }

            // ËÆ°ÁÆóÂΩìÂâçÂ∫îËØ•ÊòæÁ§∫ÁöÑ mask token ËåÉÂõ¥
            // ÂΩìÂâçÊâπÊ¨°ÁöÑËµ∑Âßã‰ΩçÁΩÆ = Â∑≤ÁîüÊàêÊï∞ÈáèÂêë‰∏ãÂèñÊï¥Âà∞ BLOCK_SIZE ÁöÑÂÄçÊï∞
            const currentBatchStart = Math.floor(revealed / 4) * 4;
            const currentBatchEnd = currentBatchStart + 4;

            let outputTokenIndex = 0; // output token ÁöÑÁ¥¢ÂºïÔºà‰∏çÂåÖÊã¨ promptÔºâ

            for (let i = 0; i < pieces.length; i++) {
                const span = document.createElement('span');
                const piece = pieces[i];
                const step = steps[i];

                if (step === -1) {
                // Prompt tokens
                span.className = 'unmasked prompt';
                span.textContent = piece;
                fragPrompt.appendChild(span);
                } else {
                // Output tokens
                if (step <= t) {
                    // Â∑≤ÁªèÁîüÊàêÁöÑ token
                    span.className = 'unmasked' + (step === t ? ' new' : '');
                    span.textContent = piece;
                    fragOutput.appendChild(span);
                } else {
                    // Êú™ÁîüÊàêÁöÑ token - Ê£ÄÊü•ÊòØÂê¶Âú®ÂΩìÂâçÊâπÊ¨°ÁöÑ mask ËåÉÂõ¥ÂÜÖ
                    if (outputTokenIndex >= currentBatchStart && outputTokenIndex < currentBatchEnd) {
                        span.className = 'masked maskToken' + (isSpec[i] ? ' special' : '');
                        span.textContent = '|<MASK>|';
                        fragOutput.appendChild(span);
                    }
                }
                outputTokenIndex++;
                }
            }

            promptEl.innerHTML = '';
            promptEl.appendChild(fragPrompt);
            outputEl.innerHTML = '';
            outputEl.appendChild(fragOutput);
            // Ëá™Âä®ÊªöÂä®Âà∞Â∫ïÈÉ®ÔºåÊòæÁ§∫ÊúÄÊñ∞ÂÜÖÂÆπ
            outputEl.scrollTop = outputEl.scrollHeight;
            
            // revealed ÊòØÂ∑≤Êè≠Á§∫ÁöÑËæìÂá∫ token Êï∞Èáè,t ÊòØÂΩìÂâçÊ≠•Êï∞(‰ªé0ÂºÄÂßã)
            // ÊØèÊ≠•ÁîüÊàêÁöÑ token Êï∞ = revealed / (t + 1)
            meta.textContent = `Generation speed: ${(t >= 0 ? (revealed / (t + 1)).toFixed(2) : 0)} tokens/step`;
            }

            function autoPlay() {
            render(currentStep);
            currentStep++;
            if (currentStep > maxStep) {
                setTimeout(() => {
                currentStep = 0;
                autoPlay();
                }, 2000);
            } else {
                setTimeout(autoPlay, 100);
            }
            }
            autoPlay();
        </script>
        </body>
    </div>
  </div>
</section>



<!-- Method Section -->
<section class="hero is-small is-light" style="padding: 5rem 1.5rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">
      Method
    </h2>

    <div class="content has-text-justified" style="margin-bottom: 2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; font-size: 1.05rem; line-height: 1.7; color: #333;">
      <p style="margin-bottom: 1.5rem;">
        <strong style="font-weight: 600;">Aha-8B-Instruct</strong> is developed based on <strong>SDAR-8B-Chat</strong> as the base model, and trained within our open-source <strong>diffusion post-training framework</strong> consisting of two stages: 
        <em>Supervised Fine-Tuning (SFT)</em> and <em>Reinforcement Learning (RL)</em>.
      </p>

      <div style="margin-bottom: 1.5rem; padding-left: 1rem; border-left: 3px solid #3273dc;">
        <h3 style="font-size: 1.15rem; font-weight: 600; margin-bottom: 0.75rem; color: #3273dc;">
          Stage 1: Supervised Fine-Tuning (SFT)
        </h3>
        <p style="margin-bottom: 0;">
          We fine-tune the model with a generation length of <strong>2K</strong>, using a high-quality dataset of <strong>2K samples synthesized by Qwen3-Max</strong>. 
          The data are carefully filtered to ensure diversity, correctness, and sequence-length consistency with training requirements.
        </p>
      </div>

      <div style="margin-bottom: 1.5rem; padding-left: 1rem; border-left: 3px solid #48c774;">
        <h3 style="font-size: 1.15rem; font-weight: 600; margin-bottom: 0.75rem; color: #48c774;">
          Stage 2: Reinforcement Learning (RL)
        </h3>
        <p style="margin-bottom: 0;">
          We adopt the <strong>Trace-RL</strong> algorithm to optimize reasoning robustness and long-context performance. 
          The generation length is extended to <strong>8K</strong>, and we integrate a diffusion-generated step map during optimization to accelerate convergence and stabilize gradient updates.
        </p>
      </div>

      <p>
        This two-stage design enables <strong>Aha-8B-Instruct</strong> to achieve efficient long-context adaptation while preserving stability and strong generalization across mathematical reasoning benchmarks.
      </p>
    </div>
  </div>
</section>


<!-- Performance Section -->
<section class="hero is-small" style="padding: 5rem 1.5rem;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">Performance</h2>

      <div class="content-has-text-justified" style="margin-bottom: 2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; font-size: 1.05rem; line-height: 1.7;">
        <p style="margin-bottom: 1rem;">
          Compared with the base model <strong style="font-weight: 600;">SDAR-8B-Chat</strong> and other strong instruction-tuned models, 
          <strong style="font-weight: 600;">Aha-8B-Instruct</strong> achieves consistent and substantial improvements across all benchmarks. 
          On <em>MATH500</em>, our model reaches <strong style="font-weight: 600;">81.60%</strong> accuracy, surpassing the base model by 
          <strong style="font-weight: 600;">+9.75%</strong> and outperforming the previous best <strong style="font-weight: 600;">Trado-8B-Instruct</strong> (75.59%) by a large margin. 
          On <em>GSM8K</em>, <strong style="font-weight: 600;">Aha-8B-Instruct</strong> attains <strong style="font-weight: 600;">90.65%</strong>, with a <strong style="font-weight: 600;">+0.78%</strong> gain over <strong style="font-weight: 600;">SDAR-8B-Chat</strong>. 
        </p>
        <p style="margin-bottom: 1rem;">
          Notably, for the challenging <em>AIME2024</em> and <em>AIME2025</em> competitions, 
          our model achieves <strong style="font-weight: 600;">20.00%</strong> and <strong style="font-weight: 600;">19.17%</strong>, 
          improving upon the base model by <strong style="font-weight: 600;">+10.83%</strong> and <strong style="font-weight: 600;">+9.79%</strong>, respectively. 
          On <em>OlympiadBench</em>, <strong style="font-weight: 600;">Aha-8B-Instruct</strong> also shows a strong gain of <strong style="font-weight: 600;">+8.78%</strong> (44.81% vs. 36.03%), 
          demonstrating superior reasoning depth and problem-solving robustness. Remarkably, <strong style="font-weight: 600;">Aha-8B-Instruct</strong> delivers performance comparable to much larger models such as <strong style="font-weight: 600;">Qwen2.5-32B-Instruct</strong>.
        </p>
      </div>


      <style>
        /* ‰∏âÁ∫øË°®Ê†∑ÂºèÔºàÈÄÇÈÖç BulmaÔºâ */
        .three-line-table {
          border-collapse: collapse;
          width: 100%;
          font-family: "Helvetica Neue", Arial, sans-serif;
          font-size: 0.95rem;
          text-align: center;
        }
        .three-line-table th,
        .three-line-table td {
          padding: 0.55rem 0.8rem;
        }

        /* È°∂Á∫øÔºàÁ≤óÔºâ*/
        .three-line-table thead tr th {
          border-top: 2px solid #0a0a0a;
        }
        /* ‰∏≠Á∫øÔºàËæÉÁªÜÔºâ*/
        .three-line-table thead {
          border-bottom: 1px solid #0a0a0a;
        }
        /* Â∫ïÁ∫øÔºàÁ≤óÔºâ*/
        .three-line-table tbody tr:last-child td {
          border-bottom: 2px solid #0a0a0a;
        }

        /* ÂéªÊéâÁ´ñÁ∫øÊïàÊûú ‚Äî‚Äî Âè™‰øùÁïô‰∏âÊù°Ê®™Á∫øÔºàÈªòËÆ§Ê≤°ÊúâÁ´ñÁ∫øÔºâ */
        .three-line-table th,
        .three-line-table td {
          border-left: none;
          border-right: none;
        }

        /* Â∞èÂØπÈΩê‰∏éÂ≠ó‰ΩìÊ†∑Âºè */
        .three-line-table thead th {
          font-weight: 600;
          padding-top: 0.9rem;
          padding-bottom: 0.6rem;
        }
        .three-line-table tbody td {
          color: #222;
        }

        /* È´ò‰∫Æ ours Ë°åÔºåÂπ∂Âä†Á≤óÂÖ∂ÊúÄ‰Ω≥ÂÄº */
        .three-line-table .ours {
          background: rgba(90,160,80,0.06);
        }
        .three-line-table .best {
          font-weight: 700;
        }

        /* ÂìçÂ∫îÂºèÔºöÂ∞èÂ±èÂπïÂ≠ó‰ΩìÂæÆË∞É */
        @media (max-width: 640px) {
          .three-line-table th, .three-line-table td { padding: 0.45rem 0.4rem; font-size: 0.88rem; }
        }
      </style>

      <div class="table-wrapper" style="margin-top:1rem;">
        <table class="three-line-table">
          <thead>
            <tr>
              <th>Model</th>
              <th>MATH500</th>
              <th>GSM8K</th>
              <th>AIME2024</th>
              <th>AIME2025</th>
              <th>OlympiadBench</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Qwen2.5-7B-Instruct</td>
              <td>73.78</td>
              <td>89.78</td>
              <td>8.96</td>
              <td>5.63</td>
              <td>36.58</td>
            </tr>
            <tr>
              <td>Qwen2.5-32B-Instruct</td>
              <td>81.13</td>
              <td class="best">94.03</td>
              <td>12.92</td>
              <td>11.88</td>
              <td class="best">45.65</td>
            </tr>
            <tr>
              <td>SDAR-8B-Chat</td>
              <td>71.85</td>
              <td>89.87</td>
              <td>9.17</td>
              <td>9.38</td>
              <td>36.03</td>
            </tr>
            <tr>
              <td>Trado-8B-Instruct</td>
              <td>75.59</td>
              <td>91.06</td>
              <td>11.67</td>
              <td>15.00</td>
              <td>40.32</td>
            </tr>
            <tr class="ours">
              <td><strong>Aha-8B-Instruct (ours)</strong></td>
              <td class="best">81.60</td>
              <td>90.65</td>
              <td class="best">20.00</td>
              <td class="best">19.17</td>
              <td>44.81</td>
            </tr>
          </tbody>
        </table>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End Performance Section -->
